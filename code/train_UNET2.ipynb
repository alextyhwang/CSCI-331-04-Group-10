{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce6faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/v/Library/Python/3.12/lib/python/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/v/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/v/Library/Python/3.12/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/v/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install tensorflow opencv-python scikit-learn matplotlib pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b0f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b126093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_directories(data_path):\n",
    "    \"\"\"Find train/val/test directories in the data path\"\"\"\n",
    "    possible_paths = [\n",
    "        os.path.join(data_path, 'train'),\n",
    "        os.path.join(data_path, 'Train'),\n",
    "        data_path\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found directory: {path}\")\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"Could not find data directories in {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebd2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_annotations(img_path, img_shape):\n",
    "    \"\"\"Convert YOLO format to segmentation mask\"\"\"\n",
    "    annotation_path = img_path.replace('images', 'labels').replace('.jpg', '.txt')\n",
    "    \n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    if os.path.exists(annotation_path):\n",
    "        try:\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            for line in lines:\n",
    "                data = line.strip().split()\n",
    "                if len(data) >= 5:\n",
    "                    x_center, y_center, width, height = map(float, data[1:5])\n",
    "                    \n",
    "                    h, w = img_shape[:2]\n",
    "                    x_center *= w\n",
    "                    y_center *= h\n",
    "                    width *= w\n",
    "                    height *= h\n",
    "                    \n",
    "                    x1 = max(0, int(x_center - width/2))\n",
    "                    y1 = max(0, int(y_center - height/2))\n",
    "                    x2 = min(w, int(x_center + width/2))\n",
    "                    y2 = min(h, int(y_center + height/2))\n",
    "                    \n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        mask[y1:y2, x1:x2] = 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation {annotation_path}: {e}\")\n",
    "                \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a1959af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_complete_dataset(data_path, img_size=(256, 256)):\n",
    "    \"\"\"Load train, validation, and test datasets\"\"\"\n",
    "    \n",
    "    def load_split(split_path):\n",
    "        \"\"\"Load images and masks from a specific split (train/valid/test)\"\"\"\n",
    "        images_path = os.path.join(split_path, 'images')\n",
    "        labels_path = os.path.join(split_path, 'labels')\n",
    "        \n",
    "        if not os.path.exists(images_path):\n",
    "            print(f\"Warning: {images_path} does not exist\")\n",
    "            return [], []\n",
    "        \n",
    "        image_files = glob.glob(os.path.join(images_path, '*.jpg')) + glob.glob(os.path.join(images_path, '*.png'))\n",
    "        \n",
    "        images = []\n",
    "        masks = []\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            try:\n",
    "                # Load image\n",
    "                image = cv2.imread(img_file)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                    \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, img_size)\n",
    "                image = image.astype(np.float32) / 255.0\n",
    "                \n",
    "                # Load corresponding mask from YOLO annotations\n",
    "                mask = load_yolo_annotations(img_file, image.shape)\n",
    "                mask = cv2.resize(mask, img_size, interpolation=cv2.INTER_NEAREST)\n",
    "                mask = mask.astype(np.float32)\n",
    "                \n",
    "                images.append(image)\n",
    "                masks.append(mask)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        images = np.array(images)\n",
    "        masks = np.array(masks)\n",
    "        masks = np.expand_dims(masks, -1)\n",
    "        \n",
    "        print(f\"Loaded {len(images)} samples from {split_path}\")\n",
    "        return images, masks\n",
    "    \n",
    "    # Load all splits\n",
    "    train_path = os.path.join(data_path, 'train')\n",
    "    valid_path = os.path.join(data_path, 'valid') \n",
    "    test_path = os.path.join(data_path, 'test')\n",
    "    \n",
    "    print(\"Loading dataset splits...\")\n",
    "    X_train, y_train = load_split(train_path)\n",
    "    X_valid, y_valid = load_split(valid_path)\n",
    "    X_test, y_test = load_split(test_path)\n",
    "    \n",
    "    return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7957b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet_model(input_size=(256, 256, 3), learning_rate=0.001, optimizer='adam'):\n",
    "    \"\"\"Create U-Net model with configurable hyperparameters\"\"\"\n",
    "    inputs = keras.Input(input_size)\n",
    "    \n",
    "    # Downsample\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x1 = x\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x2 = x\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Upsample\n",
    "    x = layers.Conv2DTranspose(64, 2, strides=2, padding='same')(x)\n",
    "    x = layers.concatenate([x, x2])\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(32, 2, strides=2, padding='same')(x)\n",
    "    x = layers.concatenate([x, x1])\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5bab7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(y_true, y_pred):\n",
    "    \"\"\"Calculate Intersection over Union\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(np.float32)\n",
    "    intersection = np.sum(y_true * y_pred_binary)\n",
    "    union = np.sum(y_true) + np.sum(y_pred_binary) - intersection\n",
    "    return intersection / (union + 1e-7)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate precision, recall, and F1-score\"\"\"\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = (y_pred.flatten() > 0.5).astype(np.int32)\n",
    "    \n",
    "    if np.sum(y_true_flat) == 0 and np.sum(y_pred_flat) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    \n",
    "    precision = precision_score(y_true_flat, y_pred_flat, zero_division=0)\n",
    "    recall = recall_score(y_true_flat, y_pred_flat, zero_division=0)\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, zero_division=0)\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e16c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cross_validation(data_path, n_splits=3, epochs=5, batch_size=8, \n",
    "                              learning_rate=0.001, optimizer='adam', max_samples=500):  # ← ADD THIS\n",
    "    \"\"\"Run K-Fold Cross Validation using only training data\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXPERIMENT 1: K-FOLD CROSS VALIDATION (Training Data Only)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load only training data for K-Fold - WITH LIMIT\n",
    "    (X_train_full, y_train_full), _, _ = load_complete_dataset(data_path)\n",
    "    \n",
    "    # ↓ ADD THESE LINES TO LIMIT DATASET SIZE\n",
    "    if max_samples and len(X_train_full) > max_samples:\n",
    "        X_train = X_train_full[:max_samples]\n",
    "        y_train = y_train_full[:max_samples]\n",
    "        print(f\"Using limited dataset: {len(X_train)} samples (was {len(X_train_full)})\")\n",
    "    else:\n",
    "        X_train = X_train_full\n",
    "        y_train = y_train_full\n",
    "    \n",
    "    # Initialize K-Fold on training data\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Split training data for this fold\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_unet_model(\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            validation_data=(X_fold_val, y_fold_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1  # ← Change to 1 to see progress\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = model.predict(X_fold_val, verbose=0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        iou = calculate_iou(y_fold_val, y_pred)\n",
    "        precision, recall, f1 = calculate_metrics(y_fold_val, y_pred)\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        fold_result = {\n",
    "            'fold': fold + 1,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'iou': iou,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "        fold_results.append(fold_result)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} Results:\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  IoU: {iou:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Calculate mean and std across folds\n",
    "    results_df = pd.DataFrame(fold_results)\n",
    "    mean_results = results_df.mean()\n",
    "    std_results = results_df.std()\n",
    "    \n",
    "    print(\"\\nK-Fold Cross Validation Summary (Training Data):\")\n",
    "    print(f\"Validation Accuracy: {mean_results['val_accuracy']:.4f} ± {std_results['val_accuracy']:.4f}\")\n",
    "    print(f\"IoU: {mean_results['iou']:.4f} ± {std_results['iou']:.4f}\")\n",
    "    print(f\"F1-Score: {mean_results['f1_score']:.4f} ± {std_results['f1_score']:.4f}\")\n",
    "    \n",
    "    return fold_results, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57fb4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_tuning(data_path, tuning_epochs=10):\n",
    "    \"\"\"Run hyperparameter tuning using train/validation splits\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXPERIMENT 2: HYPERPARAMETER TUNING (Train/Validation Data)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load train and validation data\n",
    "    (X_train, y_train), (X_val, y_val), _ = load_complete_dataset(data_path)\n",
    "    \n",
    "    hyperparameters = {\n",
    "        'learning_rate': [0.001, 0.0005],\n",
    "        'batch_size': [16, 32],\n",
    "        'optimizer': ['adam']\n",
    "    }\n",
    "    \n",
    "    print(f\"Using {len(X_train)} training samples, {len(X_val)} validation samples\")\n",
    "    \n",
    "    tuning_results = []\n",
    "    experiment_number = 1\n",
    "    total_experiments = len(hyperparameters['learning_rate']) * len(hyperparameters['batch_size']) * len(hyperparameters['optimizer'])\n",
    "    \n",
    "    for lr in hyperparameters['learning_rate']:\n",
    "        for batch_size in hyperparameters['batch_size']:\n",
    "            for optimizer in hyperparameters['optimizer']:\n",
    "                \n",
    "                print(f\"Experiment {experiment_number}/{total_experiments}\")\n",
    "                print(f\"  Learning Rate: {lr}, Batch Size: {batch_size}, Optimizer: {optimizer}\")\n",
    "                \n",
    "                model = create_unet_model(\n",
    "                    learning_rate=lr,\n",
    "                    optimizer=optimizer\n",
    "                )\n",
    "                \n",
    "                # Train on training data, validate on validation data\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=tuning_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                y_pred = model.predict(X_val, verbose=0)\n",
    "                \n",
    "                iou = calculate_iou(y_val, y_pred)\n",
    "                precision, recall, f1 = calculate_metrics(y_val, y_pred)\n",
    "                final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "                \n",
    "                result = {\n",
    "                    'experiment': experiment_number,\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'optimizer': optimizer,\n",
    "                    'final_val_accuracy': final_val_accuracy,\n",
    "                    'iou': iou,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1\n",
    "                }\n",
    "                tuning_results.append(result)\n",
    "                \n",
    "                print(f\"  Results - Val Accuracy: {final_val_accuracy:.4f}, F1: {f1:.4f}, IoU: {iou:.4f}\")\n",
    "                \n",
    "                experiment_number += 1\n",
    "    \n",
    "    # Find best configuration\n",
    "    tuning_df = pd.DataFrame(tuning_results)\n",
    "    best_by_f1 = tuning_df.loc[tuning_df['f1_score'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nBest Configuration - F1: {best_by_f1['f1_score']:.4f}\")\n",
    "    print(f\"LR: {best_by_f1['learning_rate']}, Batch: {best_by_f1['batch_size']}, Optimizer: {best_by_f1['optimizer']}\")\n",
    "    \n",
    "    return tuning_results, tuning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f7b53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment_results(kfold_results, hyperparam_results):\n",
    "    \"\"\"Plot results from both experiments\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Traffic Sign Detection: Experimental Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    kfold_df = pd.DataFrame(kfold_results)\n",
    "    hyperparam_df = pd.DataFrame(hyperparam_results)\n",
    "    \n",
    "    # Plot 1: K-Fold Results (Boxplot)\n",
    "    metrics = ['val_accuracy', 'iou', 'precision', 'recall', 'f1_score']\n",
    "    data_to_plot = [kfold_df[metric] for metric in metrics]\n",
    "    \n",
    "    axes[0,0].boxplot(data_to_plot, labels=metrics)\n",
    "    axes[0,0].set_title('K-Fold Cross Validation Results')\n",
    "    axes[0,0].set_ylabel('Score')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Learning Rate Sensitivity\n",
    "    for optimizer in hyperparam_df['optimizer'].unique():\n",
    "        optimizer_data = hyperparam_df[hyperparam_df['optimizer'] == optimizer]\n",
    "        axes[0,1].plot(optimizer_data['learning_rate'], optimizer_data['f1_score'], \n",
    "                      marker='o', label=optimizer, linewidth=2)\n",
    "    \n",
    "    axes[0,1].set_xlabel('Learning Rate')\n",
    "    axes[0,1].set_ylabel('F1-Score')\n",
    "    axes[0,1].set_title('Learning Rate Sensitivity')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].set_xscale('log')\n",
    "    \n",
    "    # Plot 3: Batch Size Impact\n",
    "    for lr in hyperparam_df['learning_rate'].unique():\n",
    "        lr_data = hyperparam_df[hyperparam_df['learning_rate'] == lr]\n",
    "        axes[1,0].plot(lr_data['batch_size'], lr_data['f1_score'], \n",
    "                      marker='s', label=f'LR={lr}', linewidth=2)\n",
    "    \n",
    "    axes[1,0].set_xlabel('Batch Size')\n",
    "    axes[1,0].set_ylabel('F1-Score')\n",
    "    axes[1,0].set_title('Batch Size Impact')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Best Configuration Summary\n",
    "    best_config = hyperparam_df.loc[hyperparam_df['f1_score'].idxmax()]\n",
    "    axes[1,1].text(0.1, 0.8, 'BEST CONFIGURATION:', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].text(0.1, 0.6, f\"Learning Rate: {best_config['learning_rate']}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.5, f\"Batch Size: {best_config['batch_size']}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.4, f\"Optimizer: {best_config['optimizer']}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.3, f\"F1-Score: {best_config['f1_score']:.4f}\", fontsize=12, fontweight='bold')\n",
    "    axes[1,1].text(0.1, 0.2, f\"IoU: {best_config['iou']:.4f}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.1, f\"Accuracy: {best_config['final_val_accuracy']:.4f}\", fontsize=12)\n",
    "    axes[1,1].set_xlim(0, 1)\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].set_title('Optimal Hyperparameters')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experimental_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ebe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_model(data_path, best_params):\n",
    "    \"\"\"Evaluate the best model on the test set\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL MODEL EVALUATION (Test Data)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load all data\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_complete_dataset(data_path)\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Combine train + val for final training\n",
    "    X_combined = np.concatenate([X_train, X_val])\n",
    "    y_combined = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    final_model = create_unet_model(\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        # batch_size=best_params['batch_size'],\n",
    "        optimizer=best_params['optimizer']\n",
    "    )\n",
    "    \n",
    "    # Train on combined train+val data\n",
    "    history = final_model.fit(\n",
    "        X_combined, y_combined,\n",
    "        epochs=30,  # You can adjust this\n",
    "        batch_size=best_params['batch_size'],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = final_model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    iou = calculate_iou(y_test, y_pred)\n",
    "    precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    test_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "    print(\"\\nFINAL TEST RESULTS:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"IoU: {iou:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model.save('final_traffic_sign_model.h5')\n",
    "    print(\"Final model saved as 'final_traffic_sign_model.h5'\")\n",
    "    \n",
    "    return final_model, (test_accuracy, iou, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a38a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     \"\"\"Main function to run all experiments\"\"\"\n",
    "#     data_path = \"/Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car\"  # This should point to the parent directory containing train/valid/test\n",
    "    \n",
    "#     print(\"COMPREHENSIVE TRAFFIC SIGN DETECTION EXPERIMENTS\")\n",
    "#     print(\"=\" * 70)\n",
    "    \n",
    "#     try:\n",
    "#         # EXPERIMENT 1: K-Fold Cross Validation (training data only)\n",
    "#         print(\"1. Running K-Fold Cross Validation...\")\n",
    "#         kfold_results, kfold_df = run_kfold_cross_validation(\n",
    "#         data_path=data_path,\n",
    "#         n_splits=3,           # Make sure this is 3, not 5\n",
    "#         epochs=5,\n",
    "#         batch_size=8,\n",
    "#         learning_rate=0.001,\n",
    "#         optimizer='adam',\n",
    "#         max_samples=500       # ← ADD THIS - use only 500 samples\n",
    "#     )\n",
    "        \n",
    "#         # EXPERIMENT 2: Hyperpafixrameter Tuning (train/validation data)\n",
    "#         print(\"2. Running Hyperparameter Tuning...\")\n",
    "#         hyperparam_results, hyperparam_df = run_hyperparameter_tuning(\n",
    "#             data_path=data_path,\n",
    "#             tuning_epochs=15\n",
    "#         )\n",
    "        \n",
    "#         # EXPERIMENT 3: Final Test Evaluation\n",
    "#         print(\"3. Running Final Test Evaluation...\")\n",
    "#         best_params = hyperparam_df.loc[hyperparam_df['f1_score'].idxmax()]\n",
    "#         final_model, test_results = evaluate_final_model(data_path, best_params)\n",
    "        \n",
    "#         # Save results\n",
    "#         kfold_df.to_csv('kfold_results.csv', index=False)\n",
    "#         hyperparam_df.to_csv('hyperparameter_results.csv', index=False)\n",
    "        \n",
    "#         print(\"\\nEXPERIMENTS COMPLETE\")\n",
    "#         print(f\"Final Test F1-Score: {test_results[4]:.4f}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2afbc91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using YOLO dataset located at:\n",
      "/Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car\n"
     ]
    }
   ],
   "source": [
    "# === GLOBAL DATA PATH ===\n",
    "DATA_PATH = \"/Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car\"\n",
    "\n",
    "print(\"Using YOLO dataset located at:\")\n",
    "print(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd6df31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_kfold():\n",
    "    \"\"\"\n",
    "    Runs ONLY the K-Fold cross-validation.\n",
    "    Saves: kfold_results.csv\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXPERIMENT 1: K-FOLD CROSS VALIDATION ===\\n\")\n",
    "\n",
    "    kfold_results, kfold_df = run_kfold_cross_validation(\n",
    "        data_path=DATA_PATH,\n",
    "        n_splits=3,\n",
    "        epochs=5,\n",
    "        batch_size=8,\n",
    "        learning_rate=0.001,\n",
    "        optimizer='adam',\n",
    "        max_samples=500\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    kfold_df.to_csv(\"kfold_results.csv\", index=False)\n",
    "    print(\"\\nSaved K-Fold results → kfold_results.csv\")\n",
    "\n",
    "    display(kfold_df)\n",
    "    return kfold_results, kfold_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e22894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT 1: K-FOLD CROSS VALIDATION ===\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 1: K-FOLD CROSS VALIDATION (Training Data Only)\n",
      "======================================================================\n",
      "Loading dataset splits...\n",
      "Loaded 3530 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/train\n",
      "Loaded 801 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/valid\n",
      "Loaded 638 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/test\n",
      "Using limited dataset: 500 samples (was 3530)\n",
      "Fold 1/3\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.7326 - loss: 0.5723 - val_accuracy: 0.7545 - val_loss: 0.5118\n",
      "Epoch 2/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 1s/step - accuracy: 0.7422 - loss: 0.5237 - val_accuracy: 0.7545 - val_loss: 0.4942\n",
      "Epoch 3/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.7893 - loss: 0.4951 - val_accuracy: 0.8171 - val_loss: 0.4450\n",
      "Epoch 4/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.8002 - loss: 0.4740 - val_accuracy: 0.8173 - val_loss: 0.4434\n",
      "Epoch 5/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.8110 - loss: 0.4596 - val_accuracy: 0.8159 - val_loss: 0.4465\n",
      "Fold 1 Results:\n",
      "  Val Accuracy: 0.8159\n",
      "  IoU: 0.2724\n",
      "  F1-Score: 0.4282\n",
      "Fold 2/3\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.7537 - loss: 0.5518 - val_accuracy: 0.7817 - val_loss: 0.5156\n",
      "Epoch 2/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 1s/step - accuracy: 0.8153 - loss: 0.4719 - val_accuracy: 0.7861 - val_loss: 0.4992\n",
      "Epoch 3/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1s/step - accuracy: 0.8353 - loss: 0.4341 - val_accuracy: 0.8204 - val_loss: 0.4558\n",
      "Epoch 4/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.8497 - loss: 0.3771 - val_accuracy: 0.8323 - val_loss: 0.3787\n",
      "Epoch 5/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 1s/step - accuracy: 0.8559 - loss: 0.3464 - val_accuracy: 0.8471 - val_loss: 0.3853\n",
      "Fold 2 Results:\n",
      "  Val Accuracy: 0.8471\n",
      "  IoU: 0.4889\n",
      "  F1-Score: 0.6568\n",
      "Fold 3/3\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.7556 - loss: 0.5629 - val_accuracy: 0.7846 - val_loss: 0.4886\n",
      "Epoch 2/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 1s/step - accuracy: 0.7957 - loss: 0.5038 - val_accuracy: 0.8089 - val_loss: 0.4562\n",
      "Epoch 3/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.8101 - loss: 0.4727 - val_accuracy: 0.8212 - val_loss: 0.4325\n",
      "Epoch 4/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 1s/step - accuracy: 0.8167 - loss: 0.4569 - val_accuracy: 0.8528 - val_loss: 0.3947\n",
      "Epoch 5/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.8336 - loss: 0.4110 - val_accuracy: 0.8625 - val_loss: 0.3463\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x169167ce0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Fold 3 Results:\n",
      "  Val Accuracy: 0.8625\n",
      "  IoU: 0.5373\n",
      "  F1-Score: 0.6990\n",
      "\n",
      "K-Fold Cross Validation Summary (Training Data):\n",
      "Validation Accuracy: 0.8418 ± 0.0238\n",
      "IoU: 0.4329 ± 0.1410\n",
      "F1-Score: 0.5947 ± 0.1457\n",
      "\n",
      "Saved K-Fold results → kfold_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>iou</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.815859</td>\n",
       "      <td>0.446530</td>\n",
       "      <td>0.272437</td>\n",
       "      <td>0.901002</td>\n",
       "      <td>0.280844</td>\n",
       "      <td>0.428213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.847055</td>\n",
       "      <td>0.385252</td>\n",
       "      <td>0.488932</td>\n",
       "      <td>0.800299</td>\n",
       "      <td>0.556873</td>\n",
       "      <td>0.656755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.862517</td>\n",
       "      <td>0.346323</td>\n",
       "      <td>0.537279</td>\n",
       "      <td>0.802529</td>\n",
       "      <td>0.619130</td>\n",
       "      <td>0.699000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fold  val_accuracy  val_loss       iou  precision    recall  f1_score\n",
       "0     1      0.815859  0.446530  0.272437   0.901002  0.280844  0.428213\n",
       "1     2      0.847055  0.385252  0.488932   0.800299  0.556873  0.656755\n",
       "2     3      0.862517  0.346323  0.537279   0.802529  0.619130  0.699000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfold_results, kfold_df = experiment_kfold()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1c32a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_hyperparameter_tuning():\n",
    "    \"\"\"\n",
    "    Runs ONLY hyperparameter tuning.\n",
    "    Saves: hyperparameter_results.csv\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXPERIMENT 2: HYPERPARAMETER TUNING ===\\n\")\n",
    "\n",
    "    hyper_results, hyper_df = run_hyperparameter_tuning(\n",
    "        data_path=DATA_PATH,\n",
    "        tuning_epochs=15\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    hyper_df.to_csv(\"hyperparameter_results.csv\", index=False)\n",
    "    print(\"\\nSaved Hyperparameter Tuning results → hyperparameter_results.csv\")\n",
    "\n",
    "    display(hyper_df)\n",
    "    return hyper_results, hyper_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5fbffd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT 2: HYPERPARAMETER TUNING ===\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: HYPERPARAMETER TUNING (Train/Validation Data)\n",
      "======================================================================\n",
      "Loading dataset splits...\n",
      "Loaded 3530 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/train\n",
      "Loaded 801 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/valid\n",
      "Loaded 638 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/test\n",
      "Using 3530 training samples, 801 validation samples\n",
      "Experiment 1/4\n",
      "  Learning Rate: 0.001, Batch Size: 16, Optimizer: adam\n",
      "  Results - Val Accuracy: 0.9527, F1: 0.9081, IoU: 0.8316\n",
      "Experiment 2/4\n",
      "  Learning Rate: 0.001, Batch Size: 32, Optimizer: adam\n",
      "  Results - Val Accuracy: 0.9392, F1: 0.8876, IoU: 0.7979\n",
      "Experiment 3/4\n",
      "  Learning Rate: 0.0005, Batch Size: 16, Optimizer: adam\n",
      "  Results - Val Accuracy: 0.9513, F1: 0.9047, IoU: 0.8259\n",
      "Experiment 4/4\n",
      "  Learning Rate: 0.0005, Batch Size: 32, Optimizer: adam\n",
      "  Results - Val Accuracy: 0.9397, F1: 0.8875, IoU: 0.7978\n",
      "\n",
      "Best Configuration - F1: 0.9081\n",
      "LR: 0.001, Batch: 16, Optimizer: adam\n",
      "\n",
      "Saved Hyperparameter Tuning results → hyperparameter_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>final_val_accuracy</th>\n",
       "      <th>iou</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>16</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.952721</td>\n",
       "      <td>0.831619</td>\n",
       "      <td>0.915960</td>\n",
       "      <td>0.900315</td>\n",
       "      <td>0.908070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939248</td>\n",
       "      <td>0.797857</td>\n",
       "      <td>0.853447</td>\n",
       "      <td>0.924523</td>\n",
       "      <td>0.887564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>16</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.951345</td>\n",
       "      <td>0.825923</td>\n",
       "      <td>0.919757</td>\n",
       "      <td>0.890058</td>\n",
       "      <td>0.904664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>32</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.939741</td>\n",
       "      <td>0.797817</td>\n",
       "      <td>0.860090</td>\n",
       "      <td>0.916800</td>\n",
       "      <td>0.887540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experiment  learning_rate  batch_size optimizer  final_val_accuracy  \\\n",
       "0           1         0.0010          16      adam            0.952721   \n",
       "1           2         0.0010          32      adam            0.939248   \n",
       "2           3         0.0005          16      adam            0.951345   \n",
       "3           4         0.0005          32      adam            0.939741   \n",
       "\n",
       "        iou  precision    recall  f1_score  \n",
       "0  0.831619   0.915960  0.900315  0.908070  \n",
       "1  0.797857   0.853447  0.924523  0.887564  \n",
       "2  0.825923   0.919757  0.890058  0.904664  \n",
       "3  0.797817   0.860090  0.916800  0.887540  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyper_results, hyper_df = experiment_hyperparameter_tuning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9fc878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def experiment_final_test(hyper_df):\n",
    "    \"\"\"\n",
    "    Runs ONLY final test evaluation.\n",
    "    Saves: final_test_results.json\n",
    "    \"\"\"\n",
    "    print(\"\\n=== EXPERIMENT 3: FINAL TEST EVALUATION ===\\n\")\n",
    "\n",
    "    best_params = hyper_df.loc[hyper_df['f1_score'].idxmax()]\n",
    "    print(best_params)\n",
    "    final_model, test_results = evaluate_final_model(DATA_PATH, best_params)\n",
    "\n",
    "    # Save test results as JSON\n",
    "    results_dict = {\n",
    "        \"iou\": float(test_results[0]),\n",
    "        \"precision\": float(test_results[1]),\n",
    "        \"recall\": float(test_results[2]),\n",
    "        \"accuracy\": float(test_results[3]),\n",
    "        \"f1_score\": float(test_results[4])\n",
    "    }\n",
    "\n",
    "    with open(\"final_test_results.json\", \"w\") as f:\n",
    "        json.dump(results_dict, f, indent=4)\n",
    "\n",
    "    print(\"\\nSaved Final Test Results → final_test_results.json\")\n",
    "\n",
    "    print(f\"\\nTest IoU:       {test_results[0]:.4f}\")\n",
    "    print(f\"Test Precision: {test_results[1]:.4f}\")\n",
    "    print(f\"Test Recall:    {test_results[2]:.4f}\")\n",
    "    print(f\"Test F1 Score:  {test_results[4]:.4f}\")\n",
    "\n",
    "    return final_model, test_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5d8abff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXPERIMENT 3: FINAL TEST EVALUATION ===\n",
      "\n",
      "experiment                   1\n",
      "learning_rate            0.001\n",
      "batch_size                  16\n",
      "optimizer                 adam\n",
      "final_val_accuracy    0.952721\n",
      "iou                   0.831619\n",
      "precision              0.91596\n",
      "recall                0.900315\n",
      "f1_score               0.90807\n",
      "Name: 0, dtype: object\n",
      "\n",
      "======================================================================\n",
      "FINAL MODEL EVALUATION (Test Data)\n",
      "======================================================================\n",
      "Loading dataset splits...\n",
      "Loaded 3530 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/train\n",
      "Loaded 801 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/valid\n",
      "Loaded 638 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/test\n",
      "Dataset sizes - Train: 3530, Val: 801, Test: 638\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_unet_model() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_model, test_results = \u001b[43mexperiment_final_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyper_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mexperiment_final_test\u001b[39m\u001b[34m(hyper_df)\u001b[39m\n\u001b[32m     10\u001b[39m best_params = hyper_df.loc[hyper_df[\u001b[33m'\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m'\u001b[39m].idxmax()]\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(best_params)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m final_model, test_results = \u001b[43mevaluate_final_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Save test results as JSON\u001b[39;00m\n\u001b[32m     15\u001b[39m results_dict = {\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33miou\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[32m0\u001b[39m]),\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[32m1\u001b[39m]),\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(test_results[\u001b[32m4\u001b[39m])\n\u001b[32m     21\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mevaluate_final_model\u001b[39m\u001b[34m(data_path, best_params)\u001b[39m\n\u001b[32m     14\u001b[39m y_combined = np.concatenate([y_train, y_val])\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Train final model with best parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m final_model = \u001b[43mcreate_unet_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlearning_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moptimizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Train on combined train+val data\u001b[39;00m\n\u001b[32m     24\u001b[39m history = final_model.fit(\n\u001b[32m     25\u001b[39m     X_combined, y_combined,\n\u001b[32m     26\u001b[39m     epochs=\u001b[32m30\u001b[39m,  \u001b[38;5;66;03m# You can adjust this\u001b[39;00m\n\u001b[32m     27\u001b[39m     batch_size=best_params[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     28\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m     29\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: create_unet_model() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "final_model, test_results = experiment_final_test(hyper_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
