{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ce6faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.2)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/v/Library/Python/3.12/lib/python/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/v/Library/Python/3.12/lib/python/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.74.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.11.1)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/v/Library/Python/3.12/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/v/Library/Python/3.12/lib/python/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install tensorflow opencv-python scikit-learn matplotlib pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b0f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b126093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_data_directories(data_path):\n",
    "    \"\"\"Find train/val/test directories in the data path\"\"\"\n",
    "    possible_paths = [\n",
    "        os.path.join(data_path, 'train'),\n",
    "        os.path.join(data_path, 'Train'),\n",
    "        data_path\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Found directory: {path}\")\n",
    "            return path\n",
    "    raise FileNotFoundError(f\"Could not find data directories in {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ebd2c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yolo_annotations(img_path, img_shape):\n",
    "    \"\"\"Convert YOLO format to segmentation mask\"\"\"\n",
    "    annotation_path = img_path.replace('images', 'labels').replace('.jpg', '.txt')\n",
    "    \n",
    "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    if os.path.exists(annotation_path):\n",
    "        try:\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            for line in lines:\n",
    "                data = line.strip().split()\n",
    "                if len(data) >= 5:\n",
    "                    x_center, y_center, width, height = map(float, data[1:5])\n",
    "                    \n",
    "                    h, w = img_shape[:2]\n",
    "                    x_center *= w\n",
    "                    y_center *= h\n",
    "                    width *= w\n",
    "                    height *= h\n",
    "                    \n",
    "                    x1 = max(0, int(x_center - width/2))\n",
    "                    y1 = max(0, int(y_center - height/2))\n",
    "                    x2 = min(w, int(x_center + width/2))\n",
    "                    y2 = min(h, int(y_center + height/2))\n",
    "                    \n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        mask[y1:y2, x1:x2] = 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing annotation {annotation_path}: {e}\")\n",
    "                \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a1959af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_complete_dataset(data_path, img_size=(256, 256)):\n",
    "    \"\"\"Load train, validation, and test datasets\"\"\"\n",
    "    \n",
    "    def load_split(split_path):\n",
    "        \"\"\"Load images and masks from a specific split (train/valid/test)\"\"\"\n",
    "        images_path = os.path.join(split_path, 'images')\n",
    "        labels_path = os.path.join(split_path, 'labels')\n",
    "        \n",
    "        if not os.path.exists(images_path):\n",
    "            print(f\"Warning: {images_path} does not exist\")\n",
    "            return [], []\n",
    "        \n",
    "        image_files = glob.glob(os.path.join(images_path, '*.jpg')) + glob.glob(os.path.join(images_path, '*.png'))\n",
    "        \n",
    "        images = []\n",
    "        masks = []\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            try:\n",
    "                # Load image\n",
    "                image = cv2.imread(img_file)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                    \n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, img_size)\n",
    "                image = image.astype(np.float32) / 255.0\n",
    "                \n",
    "                # Load corresponding mask from YOLO annotations\n",
    "                mask = load_yolo_annotations(img_file, image.shape)\n",
    "                mask = cv2.resize(mask, img_size, interpolation=cv2.INTER_NEAREST)\n",
    "                mask = mask.astype(np.float32)\n",
    "                \n",
    "                images.append(image)\n",
    "                masks.append(mask)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        images = np.array(images)\n",
    "        masks = np.array(masks)\n",
    "        masks = np.expand_dims(masks, -1)\n",
    "        \n",
    "        print(f\"Loaded {len(images)} samples from {split_path}\")\n",
    "        return images, masks\n",
    "    \n",
    "    # Load all splits\n",
    "    train_path = os.path.join(data_path, 'train')\n",
    "    valid_path = os.path.join(data_path, 'valid') \n",
    "    test_path = os.path.join(data_path, 'test')\n",
    "    \n",
    "    print(\"Loading dataset splits...\")\n",
    "    X_train, y_train = load_split(train_path)\n",
    "    X_valid, y_valid = load_split(valid_path)\n",
    "    X_test, y_test = load_split(test_path)\n",
    "    \n",
    "    return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7957b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet_model(input_size=(256, 256, 3), learning_rate=0.001, optimizer='adam'):\n",
    "    \"\"\"Create U-Net model with configurable hyperparameters\"\"\"\n",
    "    inputs = keras.Input(input_size)\n",
    "    \n",
    "    # Downsample\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x1 = x\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x2 = x\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Upsample\n",
    "    x = layers.Conv2DTranspose(64, 2, strides=2, padding='same')(x)\n",
    "    x = layers.concatenate([x, x2])\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(32, 2, strides=2, padding='same')(x)\n",
    "    x = layers.concatenate([x, x1])\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    if optimizer == 'adam':\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5bab7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(y_true, y_pred):\n",
    "    \"\"\"Calculate Intersection over Union\"\"\"\n",
    "    y_pred_binary = (y_pred > 0.5).astype(np.float32)\n",
    "    intersection = np.sum(y_true * y_pred_binary)\n",
    "    union = np.sum(y_true) + np.sum(y_pred_binary) - intersection\n",
    "    return intersection / (union + 1e-7)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate precision, recall, and F1-score\"\"\"\n",
    "    y_true_flat = y_true.flatten()\n",
    "    y_pred_flat = (y_pred.flatten() > 0.5).astype(np.int32)\n",
    "    \n",
    "    if np.sum(y_true_flat) == 0 and np.sum(y_pred_flat) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    \n",
    "    precision = precision_score(y_true_flat, y_pred_flat, zero_division=0)\n",
    "    recall = recall_score(y_true_flat, y_pred_flat, zero_division=0)\n",
    "    f1 = f1_score(y_true_flat, y_pred_flat, zero_division=0)\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e16c074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold_cross_validation(data_path, n_splits=3, epochs=5, batch_size=8, \n",
    "                              learning_rate=0.001, optimizer='adam', max_samples=500):  # ← ADD THIS\n",
    "    \"\"\"Run K-Fold Cross Validation using only training data\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXPERIMENT 1: K-FOLD CROSS VALIDATION (Training Data Only)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load only training data for K-Fold - WITH LIMIT\n",
    "    (X_train_full, y_train_full), _, _ = load_complete_dataset(data_path)\n",
    "    \n",
    "    # ↓ ADD THESE LINES TO LIMIT DATASET SIZE\n",
    "    if max_samples and len(X_train_full) > max_samples:\n",
    "        X_train = X_train_full[:max_samples]\n",
    "        y_train = y_train_full[:max_samples]\n",
    "        print(f\"Using limited dataset: {len(X_train)} samples (was {len(X_train_full)})\")\n",
    "    else:\n",
    "        X_train = X_train_full\n",
    "        y_train = y_train_full\n",
    "    \n",
    "    # Initialize K-Fold on training data\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X_train)):\n",
    "        print(f\"Fold {fold + 1}/{n_splits}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Split training data for this fold\n",
    "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # Create and train model\n",
    "        model = create_unet_model(\n",
    "            learning_rate=learning_rate,\n",
    "            optimizer=optimizer\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_fold_train, y_fold_train,\n",
    "            validation_data=(X_fold_val, y_fold_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1  # ← Change to 1 to see progress\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = model.predict(X_fold_val, verbose=0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        iou = calculate_iou(y_fold_val, y_pred)\n",
    "        precision, recall, f1 = calculate_metrics(y_fold_val, y_pred)\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        fold_result = {\n",
    "            'fold': fold + 1,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_loss': val_loss,\n",
    "            'iou': iou,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "        fold_results.append(fold_result)\n",
    "        \n",
    "        print(f\"Fold {fold + 1} Results:\")\n",
    "        print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"  IoU: {iou:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Calculate mean and std across folds\n",
    "    results_df = pd.DataFrame(fold_results)\n",
    "    mean_results = results_df.mean()\n",
    "    std_results = results_df.std()\n",
    "    \n",
    "    print(\"\\nK-Fold Cross Validation Summary (Training Data):\")\n",
    "    print(f\"Validation Accuracy: {mean_results['val_accuracy']:.4f} ± {std_results['val_accuracy']:.4f}\")\n",
    "    print(f\"IoU: {mean_results['iou']:.4f} ± {std_results['iou']:.4f}\")\n",
    "    print(f\"F1-Score: {mean_results['f1_score']:.4f} ± {std_results['f1_score']:.4f}\")\n",
    "    \n",
    "    return fold_results, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57fb4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_tuning(data_path, tuning_epochs=10):\n",
    "    \"\"\"Run hyperparameter tuning using train/validation splits\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXPERIMENT 2: HYPERPARAMETER TUNING (Train/Validation Data)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load train and validation data\n",
    "    (X_train, y_train), (X_val, y_val), _ = load_complete_dataset(data_path)\n",
    "    \n",
    "    hyperparameters = {\n",
    "        'learning_rate': [0.001, 0.0001, 0.0005],\n",
    "        'batch_size': [16, 32, 64],\n",
    "        'optimizer': ['adam', 'rmsprop']\n",
    "    }\n",
    "    \n",
    "    print(f\"Using {len(X_train)} training samples, {len(X_val)} validation samples\")\n",
    "    \n",
    "    tuning_results = []\n",
    "    experiment_number = 1\n",
    "    total_experiments = len(hyperparameters['learning_rate']) * len(hyperparameters['batch_size']) * len(hyperparameters['optimizer'])\n",
    "    \n",
    "    for lr in hyperparameters['learning_rate']:\n",
    "        for batch_size in hyperparameters['batch_size']:\n",
    "            for optimizer in hyperparameters['optimizer']:\n",
    "                \n",
    "                print(f\"Experiment {experiment_number}/{total_experiments}\")\n",
    "                print(f\"  Learning Rate: {lr}, Batch Size: {batch_size}, Optimizer: {optimizer}\")\n",
    "                \n",
    "                model = create_unet_model(\n",
    "                    learning_rate=lr,\n",
    "                    optimizer=optimizer\n",
    "                )\n",
    "                \n",
    "                # Train on training data, validate on validation data\n",
    "                history = model.fit(\n",
    "                    X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=tuning_epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    verbose=0\n",
    "                )\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                y_pred = model.predict(X_val, verbose=0)\n",
    "                \n",
    "                iou = calculate_iou(y_val, y_pred)\n",
    "                precision, recall, f1 = calculate_metrics(y_val, y_pred)\n",
    "                final_val_accuracy = history.history['val_accuracy'][-1]\n",
    "                \n",
    "                result = {\n",
    "                    'experiment': experiment_number,\n",
    "                    'learning_rate': lr,\n",
    "                    'batch_size': batch_size,\n",
    "                    'optimizer': optimizer,\n",
    "                    'final_val_accuracy': final_val_accuracy,\n",
    "                    'iou': iou,\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1\n",
    "                }\n",
    "                tuning_results.append(result)\n",
    "                \n",
    "                print(f\"  Results - Val Accuracy: {final_val_accuracy:.4f}, F1: {f1:.4f}, IoU: {iou:.4f}\")\n",
    "                \n",
    "                experiment_number += 1\n",
    "    \n",
    "    # Find best configuration\n",
    "    tuning_df = pd.DataFrame(tuning_results)\n",
    "    best_by_f1 = tuning_df.loc[tuning_df['f1_score'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nBest Configuration - F1: {best_by_f1['f1_score']:.4f}\")\n",
    "    print(f\"LR: {best_by_f1['learning_rate']}, Batch: {best_by_f1['batch_size']}, Optimizer: {best_by_f1['optimizer']}\")\n",
    "    \n",
    "    return tuning_results, tuning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f7b53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiment_results(kfold_results, hyperparam_results):\n",
    "    \"\"\"Plot results from both experiments\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Traffic Sign Detection: Experimental Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    kfold_df = pd.DataFrame(kfold_results)\n",
    "    hyperparam_df = pd.DataFrame(hyperparam_results)\n",
    "    \n",
    "    # Plot 1: K-Fold Results (Boxplot)\n",
    "    metrics = ['val_accuracy', 'iou', 'precision', 'recall', 'f1_score']\n",
    "    data_to_plot = [kfold_df[metric] for metric in metrics]\n",
    "    \n",
    "    axes[0,0].boxplot(data_to_plot, labels=metrics)\n",
    "    axes[0,0].set_title('K-Fold Cross Validation Results')\n",
    "    axes[0,0].set_ylabel('Score')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Learning Rate Sensitivity\n",
    "    for optimizer in hyperparam_df['optimizer'].unique():\n",
    "        optimizer_data = hyperparam_df[hyperparam_df['optimizer'] == optimizer]\n",
    "        axes[0,1].plot(optimizer_data['learning_rate'], optimizer_data['f1_score'], \n",
    "                      marker='o', label=optimizer, linewidth=2)\n",
    "    \n",
    "    axes[0,1].set_xlabel('Learning Rate')\n",
    "    axes[0,1].set_ylabel('F1-Score')\n",
    "    axes[0,1].set_title('Learning Rate Sensitivity')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].set_xscale('log')\n",
    "    \n",
    "    # Plot 3: Batch Size Impact\n",
    "    for lr in hyperparam_df['learning_rate'].unique():\n",
    "        lr_data = hyperparam_df[hyperparam_df['learning_rate'] == lr]\n",
    "        axes[1,0].plot(lr_data['batch_size'], lr_data['f1_score'], \n",
    "                      marker='s', label=f'LR={lr}', linewidth=2)\n",
    "    \n",
    "    axes[1,0].set_xlabel('Batch Size')\n",
    "    axes[1,0].set_ylabel('F1-Score')\n",
    "    axes[1,0].set_title('Batch Size Impact')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Best Configuration Summary\n",
    "    best_config = hyperparam_df.loc[hyperparam_df['f1_score'].idxmax()]\n",
    "    axes[1,1].text(0.1, 0.8, 'BEST CONFIGURATION:', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].text(0.1, 0.6, f\"Learning Rate: {best_config['learning_rate']}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.5, f\"Batch Size: {best_config['batch_size']}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.4, f\"Optimizer: {best_config['optimizer']}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.3, f\"F1-Score: {best_config['f1_score']:.4f}\", fontsize=12, fontweight='bold')\n",
    "    axes[1,1].text(0.1, 0.2, f\"IoU: {best_config['iou']:.4f}\", fontsize=12)\n",
    "    axes[1,1].text(0.1, 0.1, f\"Accuracy: {best_config['final_val_accuracy']:.4f}\", fontsize=12)\n",
    "    axes[1,1].set_xlim(0, 1)\n",
    "    axes[1,1].set_ylim(0, 1)\n",
    "    axes[1,1].set_title('Optimal Hyperparameters')\n",
    "    axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('experimental_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "967ebe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_final_model(data_path, best_params):\n",
    "    \"\"\"Evaluate the best model on the test set\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL MODEL EVALUATION (Test Data)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load all data\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_complete_dataset(data_path)\n",
    "    \n",
    "    print(f\"Dataset sizes - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
    "    \n",
    "    # Combine train + val for final training\n",
    "    X_combined = np.concatenate([X_train, X_val])\n",
    "    y_combined = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    final_model = create_unet_model(\n",
    "        learning_rate=best_params['learning_rate'],\n",
    "        batch_size=best_params['batch_size'],\n",
    "        optimizer=best_params['optimizer']\n",
    "    )\n",
    "    \n",
    "    # Train on combined train+val data\n",
    "    history = final_model.fit(\n",
    "        X_combined, y_combined,\n",
    "        epochs=30,  # You can adjust this\n",
    "        batch_size=best_params['batch_size'],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = final_model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    iou = calculate_iou(y_test, y_pred)\n",
    "    precision, recall, f1 = calculate_metrics(y_test, y_pred)\n",
    "    test_accuracy = history.history['accuracy'][-1]\n",
    "    \n",
    "    print(\"\\nFINAL TEST RESULTS:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"IoU: {iou:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Save the final model\n",
    "    final_model.save('final_traffic_sign_model.h5')\n",
    "    print(\"Final model saved as 'final_traffic_sign_model.h5'\")\n",
    "    \n",
    "    return final_model, (test_accuracy, iou, precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a38a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run all experiments\"\"\"\n",
    "    data_path = \"/Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car\"  # This should point to the parent directory containing train/valid/test\n",
    "    \n",
    "    print(\"COMPREHENSIVE TRAFFIC SIGN DETECTION EXPERIMENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        # EXPERIMENT 1: K-Fold Cross Validation (training data only)\n",
    "        print(\"1. Running K-Fold Cross Validation...\")\n",
    "        kfold_results, kfold_df = run_kfold_cross_validation(\n",
    "        data_path=data_path,\n",
    "        n_splits=3,           # Make sure this is 3, not 5\n",
    "        epochs=5,\n",
    "        batch_size=8,\n",
    "        learning_rate=0.001,\n",
    "        optimizer='adam',\n",
    "        max_samples=500       # ← ADD THIS - use only 500 samples\n",
    "    )\n",
    "        \n",
    "        # EXPERIMENT 2: Hyperparameter Tuning (train/validation data)\n",
    "        print(\"2. Running Hyperparameter Tuning...\")\n",
    "        hyperparam_results, hyperparam_df = run_hyperparameter_tuning(\n",
    "            data_path=data_path,\n",
    "            tuning_epochs=15\n",
    "        )\n",
    "        \n",
    "        # EXPERIMENT 3: Final Test Evaluation\n",
    "        print(\"3. Running Final Test Evaluation...\")\n",
    "        best_params = hyperparam_df.loc[hyperparam_df['f1_score'].idxmax()]\n",
    "        final_model, test_results = evaluate_final_model(data_path, best_params)\n",
    "        \n",
    "        # Save results\n",
    "        kfold_df.to_csv('kfold_results.csv', index=False)\n",
    "        hyperparam_df.to_csv('hyperparameter_results.csv', index=False)\n",
    "        \n",
    "        print(\"\\nEXPERIMENTS COMPLETE\")\n",
    "        print(f\"Final Test F1-Score: {test_results[4]:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4422625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPREHENSIVE TRAFFIC SIGN DETECTION EXPERIMENTS\n",
      "======================================================================\n",
      "1. Running K-Fold Cross Validation...\n",
      "======================================================================\n",
      "EXPERIMENT 1: K-FOLD CROSS VALIDATION (Training Data Only)\n",
      "======================================================================\n",
      "Loading dataset splits...\n",
      "Loaded 3530 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/train\n",
      "Loaded 801 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/valid\n",
      "Loaded 638 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/test\n",
      "Using limited dataset: 500 samples (was 3530)\n",
      "Fold 1/3\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 1s/step - accuracy: 0.7630 - loss: 0.5438 - val_accuracy: 0.7996 - val_loss: 0.4806\n",
      "Epoch 2/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 2s/step - accuracy: 0.7929 - loss: 0.4861 - val_accuracy: 0.8153 - val_loss: 0.4306\n",
      "Epoch 3/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 2s/step - accuracy: 0.8163 - loss: 0.4462 - val_accuracy: 0.8360 - val_loss: 0.4240\n",
      "Epoch 4/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 2s/step - accuracy: 0.8296 - loss: 0.4228 - val_accuracy: 0.8469 - val_loss: 0.4026\n",
      "Epoch 5/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 2s/step - accuracy: 0.8382 - loss: 0.3916 - val_accuracy: 0.8482 - val_loss: 0.3603\n",
      "Fold 1 Results:\n",
      "  Val Accuracy: 0.8482\n",
      "  IoU: 0.4356\n",
      "  F1-Score: 0.6069\n",
      "Fold 2/3\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 2s/step - accuracy: 0.7466 - loss: 0.5551 - val_accuracy: 0.7934 - val_loss: 0.5109\n",
      "Epoch 2/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 2s/step - accuracy: 0.8112 - loss: 0.4792 - val_accuracy: 0.7952 - val_loss: 0.4812\n",
      "Epoch 3/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 2s/step - accuracy: 0.8273 - loss: 0.4459 - val_accuracy: 0.8248 - val_loss: 0.4815\n",
      "Epoch 4/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - accuracy: 0.8366 - loss: 0.4201 - val_accuracy: 0.8252 - val_loss: 0.4085\n",
      "Epoch 5/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 2s/step - accuracy: 0.8489 - loss: 0.3767 - val_accuracy: 0.8351 - val_loss: 0.3852\n",
      "Fold 2 Results:\n",
      "  Val Accuracy: 0.8351\n",
      "  IoU: 0.4931\n",
      "  F1-Score: 0.6605\n",
      "Fold 3/3\n",
      "--------------------------------------------------\n",
      "Epoch 1/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.7591 - loss: 0.5495 - val_accuracy: 0.8255 - val_loss: 0.4351\n",
      "Epoch 2/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - accuracy: 0.8062 - loss: 0.4759 - val_accuracy: 0.8209 - val_loss: 0.4333\n",
      "Epoch 3/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - accuracy: 0.8231 - loss: 0.4464 - val_accuracy: 0.8571 - val_loss: 0.3834\n",
      "Epoch 4/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 2s/step - accuracy: 0.8370 - loss: 0.4043 - val_accuracy: 0.8609 - val_loss: 0.3617\n",
      "Epoch 5/5\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 2s/step - accuracy: 0.8451 - loss: 0.3601 - val_accuracy: 0.8649 - val_loss: 0.2965\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x3185a7240> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Fold 3 Results:\n",
      "  Val Accuracy: 0.8649\n",
      "  IoU: 0.6005\n",
      "  F1-Score: 0.7504\n",
      "\n",
      "K-Fold Cross Validation Summary (Training Data):\n",
      "Validation Accuracy: 0.8494 ± 0.0150\n",
      "IoU: 0.5097 ± 0.0837\n",
      "F1-Score: 0.6726 ± 0.0725\n",
      "2. Running Hyperparameter Tuning...\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: HYPERPARAMETER TUNING (Train/Validation Data)\n",
      "======================================================================\n",
      "Loading dataset splits...\n",
      "Loaded 3530 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/train\n",
      "Loaded 801 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/valid\n",
      "Loaded 638 samples from /Users/v/Desktop/Fall 2025 Syllabus/CSCI 331 Project/CSCI-331-04-Group-10/data/car/test\n",
      "Using 3530 training samples, 801 validation samples\n",
      "Experiment 1/18\n",
      "  Learning Rate: 0.001, Batch Size: 16, Optimizer: adam\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
