{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ensemble model - Combining U-Net and Mask R-CNN\n",
        "\n",
        "This notebook implements an ensemble approach that combines predictions from both U-Net and Mask R-CNN models for improved traffic sign detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = \"../data/car\"\n",
        "UNET_MODEL_PATH = \"../data/models/full_dataset_model_traffic_sign_unet.h5\"\n",
        "MASKRCNN_MODEL_PATH = \"final_maskrcnn_model.h5\"\n",
        "RESULTS_PATH = \"../data/results\"\n",
        "\n",
        "UNET_IMG_SIZE = (256, 256)\n",
        "MASKRCNN_IMG_SIZE = (128, 128)\n",
        "\n",
        "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Data path exists: {os.path.exists(DATA_PATH)}\")\n",
        "print(f\"U-Net model exists: {os.path.exists(UNET_MODEL_PATH)}\")\n",
        "print(f\"Mask R-CNN model exists: {os.path.exists(MASKRCNN_MODEL_PATH)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_yolo_annotations(img_path, img_shape):\n",
        "    annotation_path = img_path.replace('images', 'labels').replace('.jpg', '.txt').replace('.png', '.txt')\n",
        "    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n",
        "    \n",
        "    if os.path.exists(annotation_path):\n",
        "        try:\n",
        "            with open(annotation_path, 'r') as f:\n",
        "                lines = f.readlines()\n",
        "            for line in lines:\n",
        "                data = line.strip().split()\n",
        "                if len(data) >= 5:\n",
        "                    x_center, y_center, width, height = map(float, data[1:5])\n",
        "                    h, w = img_shape[:2]\n",
        "                    x_center *= w\n",
        "                    y_center *= h\n",
        "                    width *= w\n",
        "                    height *= h\n",
        "                    x1 = max(0, int(x_center - width/2))\n",
        "                    y1 = max(0, int(y_center - height/2))\n",
        "                    x2 = min(w, int(x_center + width/2))\n",
        "                    y2 = min(h, int(y_center + height/2))\n",
        "                    if x2 > x1 and y2 > y1:\n",
        "                        mask[y1:y2, x1:x2] = 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {annotation_path}: {e}\")\n",
        "    return mask\n",
        "\n",
        "def load_test_data(data_path, img_size):\n",
        "    test_path = os.path.join(data_path, 'test', 'images')\n",
        "    image_files = sorted([f for f in os.listdir(test_path) if f.endswith(('.jpg', '.png'))])\n",
        "    \n",
        "    images = []\n",
        "    masks = []\n",
        "    filenames = []\n",
        "    \n",
        "    for filename in tqdm(image_files, desc=\"Loading test data\"):\n",
        "        img_path = os.path.join(test_path, filename)\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            continue\n",
        "        \n",
        "        original_shape = image.shape\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image_resized = cv2.resize(image_rgb, img_size)\n",
        "        image_normalized = image_resized.astype(np.float32) / 255.0\n",
        "        \n",
        "        mask = load_yolo_annotations(img_path, original_shape)\n",
        "        mask_resized = cv2.resize(mask, img_size, interpolation=cv2.INTER_NEAREST)\n",
        "        \n",
        "        images.append(image_normalized)\n",
        "        masks.append(mask_resized)\n",
        "        filenames.append(filename)\n",
        "    \n",
        "    return np.array(images), np.array(masks), filenames\n",
        "\n",
        "print(\"Data loading functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "- `calculate_iou()` - Computes Intersection over Union for a single image prediction\n",
        "- `calculate_metrics()` - Returns precision, recall, F1-score, and accuracy for a single prediction\n",
        "- `evaluate_predictions()` - Batch evaluation that returns mean metrics with standard deviations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_iou(y_true, y_pred, threshold=0.5):\n",
        "    y_pred_binary = (y_pred > threshold).astype(np.float32)\n",
        "    y_true_binary = y_true.astype(np.float32)\n",
        "    \n",
        "    intersection = np.sum(y_true_binary * y_pred_binary)\n",
        "    union = np.sum(y_true_binary) + np.sum(y_pred_binary) - intersection\n",
        "    \n",
        "    if union == 0:\n",
        "        return 1.0 if np.sum(y_true_binary) == 0 else 0.0\n",
        "    return intersection / union\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, threshold=0.5):\n",
        "    y_true_flat = y_true.flatten().astype(np.int32)\n",
        "    y_pred_flat = (y_pred.flatten() > threshold).astype(np.int32)\n",
        "    \n",
        "    if np.sum(y_true_flat) == 0 and np.sum(y_pred_flat) == 0:\n",
        "        return 1.0, 1.0, 1.0, 1.0\n",
        "    \n",
        "    tp = np.sum((y_true_flat == 1) & (y_pred_flat == 1))\n",
        "    fp = np.sum((y_true_flat == 0) & (y_pred_flat == 1))\n",
        "    fn = np.sum((y_true_flat == 1) & (y_pred_flat == 0))\n",
        "    tn = np.sum((y_true_flat == 0) & (y_pred_flat == 0))\n",
        "    \n",
        "    precision = tp / (tp + fp + 1e-7)\n",
        "    recall = tp / (tp + fn + 1e-7)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-7)\n",
        "    \n",
        "    return precision, recall, f1, accuracy\n",
        "\n",
        "def evaluate_predictions(y_true_batch, y_pred_batch, threshold=0.5):\n",
        "    ious, precisions, recalls, f1s, accuracies = [], [], [], [], []\n",
        "    \n",
        "    for y_true, y_pred in zip(y_true_batch, y_pred_batch):\n",
        "        iou = calculate_iou(y_true, y_pred, threshold)\n",
        "        precision, recall, f1, accuracy = calculate_metrics(y_true, y_pred, threshold)\n",
        "        ious.append(iou)\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1s.append(f1)\n",
        "        accuracies.append(accuracy)\n",
        "    \n",
        "    return {\n",
        "        'iou': np.mean(ious),\n",
        "        'precision': np.mean(precisions),\n",
        "        'recall': np.mean(recalls),\n",
        "        'f1_score': np.mean(f1s),\n",
        "        'accuracy': np.mean(accuracies),\n",
        "        'iou_std': np.std(ious),\n",
        "        'f1_std': np.std(f1s)\n",
        "    }\n",
        "\n",
        "print(\"Metric functions defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Combines U-Net and Mask R-CNN predictions using multiple fusion strategies:\n",
        "\n",
        "- `weighted_average` - Blends predictions with tunable weights (default 60% U-Net, 40% Mask R-CNN)\n",
        "- `intersection` - High-confidence regions where both models agree\n",
        "- `union` - Conservative approach that captures all detections from either model\n",
        "- `confidence_based` - Weights each pixel by how confident each model is\n",
        "- `voting` - Detects if at least one model predicts positive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EnsembleModel:\n",
        "    def __init__(self, unet_model, maskrcnn_model, unet_size=(256, 256), maskrcnn_size=(128, 128)):\n",
        "        self.unet = unet_model\n",
        "        self.maskrcnn = maskrcnn_model\n",
        "        self.unet_size = unet_size\n",
        "        self.maskrcnn_size = maskrcnn_size\n",
        "        self.output_size = unet_size\n",
        "    \n",
        "    def _prepare_for_unet(self, images):\n",
        "        if images.shape[1:3] != self.unet_size:\n",
        "            resized = np.array([cv2.resize(img, self.unet_size) for img in images])\n",
        "            return resized\n",
        "        return images\n",
        "    \n",
        "    def _prepare_for_maskrcnn(self, images):\n",
        "        if images.shape[1:3] != self.maskrcnn_size:\n",
        "            resized = np.array([cv2.resize(img, self.maskrcnn_size) for img in images])\n",
        "            return resized\n",
        "        return images\n",
        "    \n",
        "    def _resize_predictions(self, predictions, target_size):\n",
        "        resized = np.array([cv2.resize(pred, target_size) for pred in predictions])\n",
        "        return resized\n",
        "    \n",
        "    def predict_unet(self, images):\n",
        "        prepared = self._prepare_for_unet(images)\n",
        "        preds = self.unet.predict(prepared, verbose=0)\n",
        "        if preds.shape[-1] == 1:\n",
        "            preds = preds[:, :, :, 0]\n",
        "        return preds\n",
        "    \n",
        "    def predict_maskrcnn(self, images):\n",
        "        prepared = self._prepare_for_maskrcnn(images)\n",
        "        preds = self.maskrcnn.predict(prepared, verbose=0)\n",
        "        if preds.shape[-1] == 1:\n",
        "            preds = preds[:, :, :, 0]\n",
        "        preds_resized = self._resize_predictions(preds, self.output_size)\n",
        "        return preds_resized\n",
        "    \n",
        "    def predict(self, images, strategy='weighted_average', weights=(0.6, 0.4)):\n",
        "        unet_preds = self.predict_unet(images)\n",
        "        mrcnn_preds = self.predict_maskrcnn(images)\n",
        "        \n",
        "        if strategy == 'weighted_average':\n",
        "            return weights[0] * unet_preds + weights[1] * mrcnn_preds\n",
        "        \n",
        "        elif strategy == 'intersection':\n",
        "            unet_binary = (unet_preds > 0.5).astype(np.float32)\n",
        "            mrcnn_binary = (mrcnn_preds > 0.5).astype(np.float32)\n",
        "            return unet_binary * mrcnn_binary\n",
        "        \n",
        "        elif strategy == 'union':\n",
        "            return np.maximum(unet_preds, mrcnn_preds)\n",
        "        \n",
        "        elif strategy == 'confidence_based':\n",
        "            unet_conf = np.abs(unet_preds - 0.5)\n",
        "            mrcnn_conf = np.abs(mrcnn_preds - 0.5)\n",
        "            total_conf = unet_conf + mrcnn_conf + 1e-7\n",
        "            return (unet_preds * unet_conf + mrcnn_preds * mrcnn_conf) / total_conf\n",
        "        \n",
        "        elif strategy == 'voting':\n",
        "            unet_binary = (unet_preds > 0.5).astype(np.float32)\n",
        "            mrcnn_binary = (mrcnn_preds > 0.5).astype(np.float32)\n",
        "            votes = unet_binary + mrcnn_binary\n",
        "            return (votes >= 1).astype(np.float32)\n",
        "        \n",
        "        else:\n",
        "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "    \n",
        "    def get_all_strategies(self):\n",
        "        return ['weighted_average', 'intersection', 'union', 'confidence_based', 'voting']\n",
        "\n",
        "print(\"EnsembleModel class defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading models...\")\n",
        "unet_model = keras.models.load_model(UNET_MODEL_PATH)\n",
        "print(f\"U-Net loaded: {unet_model.input_shape} -> {unet_model.output_shape}\")\n",
        "\n",
        "maskrcnn_model = keras.models.load_model(MASKRCNN_MODEL_PATH)\n",
        "print(f\"Mask R-CNN loaded: {maskrcnn_model.input_shape} -> {maskrcnn_model.output_shape}\")\n",
        "\n",
        "print(\"\\nLoading test data...\")\n",
        "X_test, y_test, filenames = load_test_data(DATA_PATH, UNET_IMG_SIZE)\n",
        "print(f\"Test data loaded: {X_test.shape[0]} images, shape {X_test.shape[1:]}\")\n",
        "\n",
        "ensemble = EnsembleModel(\n",
        "    unet_model=unet_model,\n",
        "    maskrcnn_model=maskrcnn_model,\n",
        "    unet_size=UNET_IMG_SIZE,\n",
        "    maskrcnn_size=MASKRCNN_IMG_SIZE\n",
        ")\n",
        "print(\"\\nEnsemble model initialized.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating individual models and ensemble strategies...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "print(\"\\n1. Evaluating U-Net alone...\")\n",
        "unet_preds = ensemble.predict_unet(X_test)\n",
        "all_results['U-Net'] = evaluate_predictions(y_test, unet_preds)\n",
        "print(f\"   IoU: {all_results['U-Net']['iou']:.4f}, F1: {all_results['U-Net']['f1_score']:.4f}\")\n",
        "\n",
        "print(\"\\n2. Evaluating Mask R-CNN alone...\")\n",
        "mrcnn_preds = ensemble.predict_maskrcnn(X_test)\n",
        "all_results['Mask R-CNN'] = evaluate_predictions(y_test, mrcnn_preds)\n",
        "print(f\"   IoU: {all_results['Mask R-CNN']['iou']:.4f}, F1: {all_results['Mask R-CNN']['f1_score']:.4f}\")\n",
        "\n",
        "print(\"\\n3. Evaluating ensemble strategies...\")\n",
        "for strategy in ensemble.get_all_strategies():\n",
        "    preds = ensemble.predict(X_test, strategy=strategy)\n",
        "    all_results[f'Ensemble ({strategy})'] = evaluate_predictions(y_test, preds)\n",
        "    print(f\"   {strategy}: IoU={all_results[f'Ensemble ({strategy})']['iou']:.4f}, F1={all_results[f'Ensemble ({strategy})']['f1_score']:.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(all_results).T\n",
        "results_df = results_df.round(4)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESULTS SUMMARY:\")\n",
        "print(results_df[['iou', 'f1_score', 'precision', 'recall', 'accuracy']].to_string())\n",
        "\n",
        "best_model = results_df['iou'].idxmax()\n",
        "print(f\"\\nBest model by IoU: {best_model} ({results_df.loc[best_model, 'iou']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(images, ground_truth, unet_preds, mrcnn_preds, ensemble_preds, \n",
        "                          sample_indices=[0, 50, 100, 200], save_path=None):\n",
        "    n_samples = len(sample_indices)\n",
        "    fig, axes = plt.subplots(n_samples, 6, figsize=(18, 3*n_samples))\n",
        "    \n",
        "    titles = ['Input Image', 'Ground Truth', 'U-Net', 'Mask R-CNN', 'Ensemble\\n(confidence)', 'Overlay']\n",
        "    \n",
        "    for row, idx in enumerate(sample_indices):\n",
        "        axes[row, 0].imshow(images[idx])\n",
        "        axes[row, 0].set_title(titles[0] if row == 0 else '')\n",
        "        \n",
        "        axes[row, 1].imshow(ground_truth[idx], cmap='gray')\n",
        "        axes[row, 1].set_title(titles[1] if row == 0 else '')\n",
        "        \n",
        "        axes[row, 2].imshow(unet_preds[idx], cmap='gray')\n",
        "        iou_u = calculate_iou(ground_truth[idx], unet_preds[idx])\n",
        "        axes[row, 2].set_title(f'{titles[2]}\\nIoU: {iou_u:.3f}' if row == 0 else f'IoU: {iou_u:.3f}')\n",
        "        \n",
        "        axes[row, 3].imshow(mrcnn_preds[idx], cmap='gray')\n",
        "        iou_m = calculate_iou(ground_truth[idx], mrcnn_preds[idx])\n",
        "        axes[row, 3].set_title(f'{titles[3]}\\nIoU: {iou_m:.3f}' if row == 0 else f'IoU: {iou_m:.3f}')\n",
        "        \n",
        "        axes[row, 4].imshow(ensemble_preds[idx], cmap='gray')\n",
        "        iou_e = calculate_iou(ground_truth[idx], ensemble_preds[idx])\n",
        "        axes[row, 4].set_title(f'{titles[4]}\\nIoU: {iou_e:.3f}' if row == 0 else f'IoU: {iou_e:.3f}')\n",
        "        \n",
        "        overlay = images[idx].copy()\n",
        "        pred_binary = (ensemble_preds[idx] > 0.5).astype(np.float32)\n",
        "        overlay[:,:,1] = np.clip(overlay[:,:,1] + pred_binary * 0.4, 0, 1)\n",
        "        axes[row, 5].imshow(overlay)\n",
        "        axes[row, 5].set_title(titles[5] if row == 0 else '')\n",
        "        \n",
        "        for ax in axes[row]:\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved: {save_path}\")\n",
        "    plt.show()\n",
        "\n",
        "unet_conf = np.abs(unet_preds - 0.5)\n",
        "mrcnn_conf = np.abs(mrcnn_preds - 0.5)\n",
        "ensemble_confidence_preds = (unet_preds * unet_conf + mrcnn_preds * mrcnn_conf) / (unet_conf + mrcnn_conf + 1e-7)\n",
        "\n",
        "visualize_predictions(\n",
        "    X_test, y_test, unet_preds, mrcnn_preds, ensemble_confidence_preds,\n",
        "    sample_indices=[0, 100, 300, 500],\n",
        "    save_path=os.path.join(RESULTS_PATH, 'ensemble_predictions_comparison.png')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "models = list(all_results.keys())\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6', '#f39c12', '#1abc9c', '#34495e']\n",
        "\n",
        "iou_values = [all_results[m]['iou'] for m in models]\n",
        "bars1 = axes[0].barh(models, iou_values, color=colors)\n",
        "axes[0].set_xlabel('IoU Score')\n",
        "axes[0].set_title('Model Comparison: IoU')\n",
        "axes[0].set_xlim(0, 1)\n",
        "for bar, val in zip(bars1, iou_values):\n",
        "    axes[0].text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center', fontsize=9)\n",
        "\n",
        "f1_values = [all_results[m]['f1_score'] for m in models]\n",
        "bars2 = axes[1].barh(models, f1_values, color=colors)\n",
        "axes[1].set_xlabel('F1 Score')\n",
        "axes[1].set_title('Model Comparison: F1 Score')\n",
        "axes[1].set_xlim(0, 1)\n",
        "for bar, val in zip(bars2, f1_values):\n",
        "    axes[1].text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_PATH, 'ensemble_metrics_comparison.png'), dpi=150, bbox_inches='tight')\n",
        "print(f\"Saved: {os.path.join(RESULTS_PATH, 'ensemble_metrics_comparison.png')}\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "metrics = ['IoU', 'F1', 'Precision', 'Recall', 'Accuracy']\n",
        "key_models = ['U-Net', 'Mask R-CNN', 'Ensemble (confidence_based)']\n",
        "colors_selected = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.25\n",
        "\n",
        "for i, (model, color) in enumerate(zip(key_models, colors_selected)):\n",
        "    if model in all_results:\n",
        "        values = [all_results[model]['iou'], all_results[model]['f1_score'], \n",
        "                  all_results[model]['precision'], all_results[model]['recall'], \n",
        "                  all_results[model]['accuracy']]\n",
        "        bars = axes[0].bar(x + i*width, values, width, label=model, color=color)\n",
        "\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_title('Key Models: All Metrics Comparison')\n",
        "axes[0].set_xticks(x + width)\n",
        "axes[0].set_xticklabels(metrics)\n",
        "axes[0].legend(loc='lower right')\n",
        "axes[0].set_ylim(0, 1.1)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "improvement_data = {\n",
        "    'vs U-Net': (all_results.get('Ensemble (confidence_based)', {}).get('iou', 0) - all_results.get('U-Net', {}).get('iou', 0)) * 100,\n",
        "    'vs Mask R-CNN': (all_results.get('Ensemble (confidence_based)', {}).get('iou', 0) - all_results.get('Mask R-CNN', {}).get('iou', 0)) * 100,\n",
        "}\n",
        "\n",
        "bars = axes[1].bar(improvement_data.keys(), improvement_data.values(), color=['#2ecc71', '#2ecc71'])\n",
        "axes[1].set_ylabel('IoU Improvement (%)')\n",
        "axes[1].set_title('Ensemble (confidence) Improvement Over Individual Models')\n",
        "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "for bar, val in zip(bars, improvement_data.values()):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, val + 0.3, f'+{val:.1f}%', ha='center', fontsize=11, fontweight='bold')\n",
        "axes[1].set_ylim(0, max(improvement_data.values()) + 5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(RESULTS_PATH, 'ensemble_improvement_analysis.png'), dpi=150, bbox_inches='tight')\n",
        "print(f\"Saved: {os.path.join(RESULTS_PATH, 'ensemble_improvement_analysis.png')}\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"VISUALIZATION COMPLETE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nPlots saved to: {RESULTS_PATH}/\")\n",
        "print(\"  - ensemble_predictions_comparison.png\")\n",
        "print(\"  - ensemble_metrics_comparison.png\")\n",
        "print(\"  - ensemble_improvement_analysis.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_df.to_csv(os.path.join(RESULTS_PATH, 'ensemble_results.csv'))\n",
        "\n",
        "best_ensemble = 'Ensemble (confidence_based)'\n",
        "summary = {\n",
        "    'best_model': best_model,\n",
        "    'best_iou': float(results_df.loc[best_model, 'iou']),\n",
        "    'best_f1': float(results_df.loc[best_model, 'f1_score']),\n",
        "    'unet_iou': float(all_results['U-Net']['iou']),\n",
        "    'maskrcnn_iou': float(all_results['Mask R-CNN']['iou']),\n",
        "    'improvement_over_unet': float((all_results[best_ensemble]['iou'] - all_results['U-Net']['iou']) * 100),\n",
        "    'improvement_over_maskrcnn': float((all_results[best_ensemble]['iou'] - all_results['Mask R-CNN']['iou']) * 100),\n",
        "}\n",
        "\n",
        "with open(os.path.join(RESULTS_PATH, 'ensemble_summary.json'), 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"ENSEMBLE MODEL EVALUATION - FINAL SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nBest Model: {best_model}\")\n",
        "print(f\"Best IoU: {summary['best_iou']:.4f}\")\n",
        "print(f\"Best F1: {summary['best_f1']:.4f}\")\n",
        "print(f\"\\nImprovement over U-Net: +{summary['improvement_over_unet']:.1f}%\")\n",
        "print(f\"Improvement over Mask R-CNN: +{summary['improvement_over_maskrcnn']:.1f}%\")\n",
        "print(\"\\nFiles saved:\")\n",
        "print(f\"  - {RESULTS_PATH}/ensemble_results.csv\")\n",
        "print(f\"  - {RESULTS_PATH}/ensemble_summary.json\")\n",
        "print(f\"  - {RESULTS_PATH}/ensemble_predictions_comparison.png\")\n",
        "print(f\"  - {RESULTS_PATH}/ensemble_metrics_comparison.png\")\n",
        "print(f\"  - {RESULTS_PATH}/ensemble_improvement_analysis.png\")\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
